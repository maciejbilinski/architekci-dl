{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tests.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#KOD"
      ],
      "metadata": {
        "id": "ryjI50qzloy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##POMOCNICZE KLASY DO WCZYTYWANIA DANYCH"
      ],
      "metadata": {
        "id": "1-6WRL5-iv0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3CqbGjYg4SC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def _get_score(x_instance, W):\n",
        "    return np.dot(W, x_instance)\n",
        "\n",
        "\n",
        "def cross_entropy_loss_function(x_instance, W, y_i, debug=False):\n",
        "    s = _get_score(x_instance, W)\n",
        "    if debug:\n",
        "        print('SCORE:', s)\n",
        "        e_s = np.e ** s\n",
        "        print('e^s: ', e_s)\n",
        "        e_s_sum = np.sum(e_s)\n",
        "        print('sum of the e^s:', e_s_sum)\n",
        "        normalized_probability = np.e ** s[y_i] / e_s_sum\n",
        "        print('normalized probability:', normalized_probability)\n",
        "        result = -np.log(normalized_probability)\n",
        "        print('loss:', result)\n",
        "        return result\n",
        "    return -np.log(np.e ** s[y_i] / np.sum(np.e ** s))\n",
        "\n",
        "\n",
        "def loss_function_for_the_dataset(x, W, y, loss_fn=cross_entropy_loss_function):\n",
        "    n = len(x)\n",
        "    loss_sum = 0.0\n",
        "    for i in range(n):\n",
        "        loss_sum += loss_fn(x[i], W, y[i])\n",
        "    return loss_sum/n\n",
        "\n",
        "\n",
        "def loss_function_for_the_dataset_with_L2(x, W, y, loss_fn=cross_entropy_loss_function, regularization_strength=0.01):\n",
        "    loss = loss_function_for_the_dataset(x, W, y, loss_fn)\n",
        "    return loss + regularization_strength * np.sum(W**2)\n",
        "\n",
        "\n",
        "# numerical gradient\n",
        "def compute_gradient(training_x, training_y, W, h=0.001, regularization_strength=0.01):\n",
        "    loss0 = loss_function_for_the_dataset_with_L2(training_x, W, training_y, regularization_strength=regularization_strength)\n",
        "    grad = np.empty(W.shape)\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            W[i, j] += h\n",
        "            loss = loss_function_for_the_dataset_with_L2(training_x, W, training_y, regularization_strength=regularization_strength)\n",
        "            grad[i, j] = (loss - loss0)/h\n",
        "            W[i, j] -= h\n",
        "    return grad\n",
        "\n",
        "\n",
        "class LinearClassifier:\n",
        "    # weight matrix is by default np.ones\n",
        "    def __init__(self, training_x, training_y, test_x, test_y, classes_length):\n",
        "        self.training_x = training_x\n",
        "        self.training_y = training_y\n",
        "        self.test_x = test_x\n",
        "        self.test_y = test_y\n",
        "        self.W = np.ones((classes_length, training_x.shape[1]))\n",
        "\n",
        "    def get_score(self, x_instance):\n",
        "        return _get_score(x_instance, self.W)\n",
        "\n",
        "    def random_search(self, iterations=1000, regularization_strength=0.01):\n",
        "        self.W = np.ones(self.W.shape)\n",
        "        best_loss = loss_function_for_the_dataset_with_L2(self.training_x, self.W, self.training_y, regularization_strength=regularization_strength)\n",
        "        for num in range(0, iterations):\n",
        "            weight_matrix = np.random.randn(*self.W.shape) * 0.001  # from Michigan Online\n",
        "            loss = loss_function_for_the_dataset_with_L2(self.training_x, weight_matrix, self.training_y, regularization_strength=regularization_strength)\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                self.W = weight_matrix\n",
        "        return self.W, best_loss\n",
        "\n",
        "    def check_accuracy(self):\n",
        "        correct = 0\n",
        "        options = [0, 0, 0]\n",
        "        for i in range(0, self.test_x.shape[0]):\n",
        "            result = self.get_score(self.test_x[i])\n",
        "            selected_class = np.argmax(result)\n",
        "            options[selected_class] += 1\n",
        "            if selected_class == self.test_y[i]:\n",
        "                correct = correct + 1\n",
        "\n",
        "        return correct / self.test_x.shape[0], options\n",
        "\n",
        "    def gradient_descent(self, learning_rate=0.01, h=0.001, iterations=1000, regularization_strength=0.01):\n",
        "        self.W = np.ones(self.W.shape)  # init weight matrix\n",
        "        for i in range(iterations):\n",
        "            grad = compute_gradient(self.training_x, self.training_y, self.W, h, regularization_strength)\n",
        "            self.W -= learning_rate * grad\n",
        "        return self.W, loss_function_for_the_dataset_with_L2(self.training_x, self.W, self.training_y, regularization_strength=regularization_strength)\n",
        "\n",
        "    def adam(self, learning_rate=0.001, h=0.001, beta1=0.9, beta2=0.999, iterations=1000, regularization_strength=0.01):\n",
        "        self.W = np.ones(self.W.shape)  # init weight matrix\n",
        "        moment1 = 0\n",
        "        moment2 = 0\n",
        "        for t in range(1, iterations + 1):\n",
        "            dw = compute_gradient(self.training_x, self.training_y, self.W, h, regularization_strength)\n",
        "            moment1 = beta1 * moment1 + (1 - beta1) * dw\n",
        "            moment2 = beta2 * moment2 + (1 - beta2) * (dw ** 2)\n",
        "            moment1_unbias = moment1 / (1 - beta1 ** t)\n",
        "            moment2_unbias = moment2 / (1 - beta2 ** t)\n",
        "            self.W -= learning_rate * moment1_unbias / (np.sqrt(moment2_unbias) + 1e-7)\n",
        "        return self.W, loss_function_for_the_dataset_with_L2(self.training_x, self.W, self.training_y, regularization_strength=regularization_strength)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FUNKCJE I KLASA ODPOWIEDZIALNA NA LINIOWY KLASYFIKATOR"
      ],
      "metadata": {
        "id": "sFbGoplQi63Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_score(x_instance, W):\n",
        "    return np.dot(W, x_instance)\n",
        "\n",
        "\n",
        "def cross_entropy_loss_function(x_instance, W, y_i, debug=False):\n",
        "    s = _get_score(x_instance, W)\n",
        "    if debug:\n",
        "        print('SCORE:', s)\n",
        "        e_s = np.e ** s\n",
        "        print('e^s: ', e_s)\n",
        "        e_s_sum = np.sum(e_s)\n",
        "        print('sum of the e^s:', e_s_sum)\n",
        "        normalized_probability = np.e ** s[y_i] / e_s_sum\n",
        "        print('normalized probability:', normalized_probability)\n",
        "        result = -np.log(normalized_probability)\n",
        "        print('loss:', result)\n",
        "        return result\n",
        "    return -np.log(np.e ** s[y_i] / np.sum(np.e ** s))\n",
        "\n",
        "\n",
        "def loss_function_for_the_dataset(x, W, y, loss_fn=cross_entropy_loss_function):\n",
        "    n = len(x)\n",
        "    loss_sum = 0.0\n",
        "    for i in range(n):\n",
        "        loss_sum += loss_fn(x[i], W, y[i])\n",
        "    return loss_sum/n\n",
        "\n",
        "\n",
        "def loss_function_for_the_dataset_with_L2(x, W, y, loss_fn=cross_entropy_loss_function, regularization_strength=0.01):\n",
        "    loss = loss_function_for_the_dataset(x, W, y, loss_fn)\n",
        "    return loss + regularization_strength * np.sum(W**2)\n",
        "\n",
        "\n",
        "# numerical gradient\n",
        "def compute_gradient(training_x, training_y, W, h=0.001, regularization_strength=0.01):\n",
        "    loss0 = loss_function_for_the_dataset_with_L2(training_x, W, training_y, regularization_strength=regularization_strength)\n",
        "    grad = np.empty(W.shape)\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            W[i, j] += h\n",
        "            loss = loss_function_for_the_dataset_with_L2(training_x, W, training_y, regularization_strength=regularization_strength)\n",
        "            grad[i, j] = (loss - loss0)/h\n",
        "            W[i, j] -= h\n",
        "    return grad\n",
        "\n",
        "\n",
        "class LinearClassifier:\n",
        "    # weight matrix is by default np.ones\n",
        "    def __init__(self, training_x, training_y, test_x, test_y, classes_length):\n",
        "        self.training_x = training_x\n",
        "        self.training_y = training_y\n",
        "        self.test_x = test_x\n",
        "        self.test_y = test_y\n",
        "        self.W = np.ones((classes_length, training_x.shape[1]))\n",
        "\n",
        "    def get_score(self, x_instance):\n",
        "        return _get_score(x_instance, self.W)\n",
        "\n",
        "    def random_search(self, iterations=1000, regularization_strength=0.01):\n",
        "        self.W = np.ones(self.W.shape)\n",
        "        best_loss = loss_function_for_the_dataset_with_L2(self.training_x, self.W, self.training_y, regularization_strength=regularization_strength)\n",
        "        for num in range(0, iterations):\n",
        "            weight_matrix = np.random.randn(*self.W.shape) * 0.001  # from Michigan Online\n",
        "            loss = loss_function_for_the_dataset_with_L2(self.training_x, weight_matrix, self.training_y, regularization_strength=regularization_strength)\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                self.W = weight_matrix\n",
        "        return self.W, best_loss\n",
        "\n",
        "    def check_accuracy(self):\n",
        "        correct = 0\n",
        "        options = [0, 0, 0]\n",
        "        for i in range(0, self.test_x.shape[0]):\n",
        "            result = self.get_score(self.test_x[i])\n",
        "            selected_class = np.argmax(result)\n",
        "            options[selected_class] += 1\n",
        "            if selected_class == self.test_y[i]:\n",
        "                correct = correct + 1\n",
        "\n",
        "        return correct / self.test_x.shape[0], options\n",
        "\n",
        "    def gradient_descent(self, learning_rate=0.01, h=0.001, iterations=1000, regularization_strength=0.01):\n",
        "        self.W = np.ones(self.W.shape)  # init weight matrix\n",
        "        for i in range(iterations):\n",
        "            grad = compute_gradient(self.training_x, self.training_y, self.W, h, regularization_strength)\n",
        "            self.W -= learning_rate * grad\n",
        "        return self.W, loss_function_for_the_dataset_with_L2(self.training_x, self.W, self.training_y, regularization_strength=regularization_strength)\n",
        "\n",
        "    def adam(self, learning_rate=0.001, h=0.001, beta1=0.9, beta2=0.999, iterations=1000, regularization_strength=0.01):\n",
        "        self.W = np.ones(self.W.shape)  # init weight matrix\n",
        "        moment1 = 0\n",
        "        moment2 = 0\n",
        "        for t in range(1, iterations + 1):\n",
        "            dw = compute_gradient(self.training_x, self.training_y, self.W, h, regularization_strength)\n",
        "            moment1 = beta1 * moment1 + (1 - beta1) * dw\n",
        "            moment2 = beta2 * moment2 + (1 - beta2) * (dw ** 2)\n",
        "            moment1_unbias = moment1 / (1 - beta1 ** t)\n",
        "            moment2_unbias = moment2 / (1 - beta2 ** t)\n",
        "            self.W -= learning_rate * moment1_unbias / (np.sqrt(moment2_unbias) + 1e-7)\n",
        "        return self.W, loss_function_for_the_dataset_with_L2(self.training_x, self.W, self.training_y, regularization_strength=regularization_strength)\n"
      ],
      "metadata": {
        "id": "0l5Vs1Avi-NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTY"
      ],
      "metadata": {
        "id": "xq7jvZBTjEOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #0 - WCZYTANIE DANYCH"
      ],
      "metadata": {
        "id": "qj8vv11njHrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = CSVReader('drive/MyDrive/ML/IRIS.csv')\n",
        "\n",
        "print('RAW DATA')\n",
        "print(iris.get_data())\n",
        "\n",
        "print('RAW DATA AS NUMPY')\n",
        "data = iris.get_numpy()\n",
        "print(data)\n",
        "\n",
        "manager = DataManager(data)\n",
        "print(\"SEPARATED X AND Y\")\n",
        "x, y = manager.separate_x_y()\n",
        "print('X')\n",
        "print(x)\n",
        "print('Y')\n",
        "print(y)\n",
        "\n",
        "print('CONVERTED X TO FLOATS')\n",
        "x = manager.convert_x_to_floats()\n",
        "print(x)\n",
        "\n",
        "print('CONVERTED CLASSES NAMES TO IDS')\n",
        "convention, y = manager.convert_classes_to_numbers()\n",
        "print(y)\n",
        "print(convention)\n",
        "classes = [x[1] for x in convention]\n",
        "\n",
        "print('BIAS TRICK')\n",
        "x = manager.add_bias_trick()\n",
        "print(x)\n",
        "\n",
        "print('TRAINING AND TEST SETS')\n",
        "training_x, training_y, test_x, test_y = manager.get_training_test_set_in_equal_portion()\n",
        "print('TRAINING SET')\n",
        "print(training_x, training_y)\n",
        "print('TEST SET (20% OF THE DATASET)')\n",
        "print(test_x, test_y)\n",
        "\n",
        "print('LINEAR CLASSIFIER')\n",
        "classifier = LinearClassifier(training_x, training_y, test_x, test_y, manager.get_classes_length())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaLae4KcjQNQ",
        "outputId": "414753e3-0ffb-4368-d6d4-27331fdd6003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW DATA\n",
            "[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'], ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa'], ['4.9', '3', '1.4', '0.2', 'Iris-setosa'], ['4.7', '3.2', '1.3', '0.2', 'Iris-setosa'], ['4.6', '3.1', '1.5', '0.2', 'Iris-setosa'], ['5', '3.6', '1.4', '0.2', 'Iris-setosa'], ['5.4', '3.9', '1.7', '0.4', 'Iris-setosa'], ['4.6', '3.4', '1.4', '0.3', 'Iris-setosa'], ['5', '3.4', '1.5', '0.2', 'Iris-setosa'], ['4.4', '2.9', '1.4', '0.2', 'Iris-setosa'], ['4.9', '3.1', '1.5', '0.1', 'Iris-setosa'], ['5.4', '3.7', '1.5', '0.2', 'Iris-setosa'], ['4.8', '3.4', '1.6', '0.2', 'Iris-setosa'], ['4.8', '3', '1.4', '0.1', 'Iris-setosa'], ['4.3', '3', '1.1', '0.1', 'Iris-setosa'], ['5.8', '4', '1.2', '0.2', 'Iris-setosa'], ['5.7', '4.4', '1.5', '0.4', 'Iris-setosa'], ['5.4', '3.9', '1.3', '0.4', 'Iris-setosa'], ['5.1', '3.5', '1.4', '0.3', 'Iris-setosa'], ['5.7', '3.8', '1.7', '0.3', 'Iris-setosa'], ['5.1', '3.8', '1.5', '0.3', 'Iris-setosa'], ['5.4', '3.4', '1.7', '0.2', 'Iris-setosa'], ['5.1', '3.7', '1.5', '0.4', 'Iris-setosa'], ['4.6', '3.6', '1', '0.2', 'Iris-setosa'], ['5.1', '3.3', '1.7', '0.5', 'Iris-setosa'], ['4.8', '3.4', '1.9', '0.2', 'Iris-setosa'], ['5', '3', '1.6', '0.2', 'Iris-setosa'], ['5', '3.4', '1.6', '0.4', 'Iris-setosa'], ['5.2', '3.5', '1.5', '0.2', 'Iris-setosa'], ['5.2', '3.4', '1.4', '0.2', 'Iris-setosa'], ['4.7', '3.2', '1.6', '0.2', 'Iris-setosa'], ['4.8', '3.1', '1.6', '0.2', 'Iris-setosa'], ['5.4', '3.4', '1.5', '0.4', 'Iris-setosa'], ['5.2', '4.1', '1.5', '0.1', 'Iris-setosa'], ['5.5', '4.2', '1.4', '0.2', 'Iris-setosa'], ['4.9', '3.1', '1.5', '0.1', 'Iris-setosa'], ['5', '3.2', '1.2', '0.2', 'Iris-setosa'], ['5.5', '3.5', '1.3', '0.2', 'Iris-setosa'], ['4.9', '3.1', '1.5', '0.1', 'Iris-setosa'], ['4.4', '3', '1.3', '0.2', 'Iris-setosa'], ['5.1', '3.4', '1.5', '0.2', 'Iris-setosa'], ['5', '3.5', '1.3', '0.3', 'Iris-setosa'], ['4.5', '2.3', '1.3', '0.3', 'Iris-setosa'], ['4.4', '3.2', '1.3', '0.2', 'Iris-setosa'], ['5', '3.5', '1.6', '0.6', 'Iris-setosa'], ['5.1', '3.8', '1.9', '0.4', 'Iris-setosa'], ['4.8', '3', '1.4', '0.3', 'Iris-setosa'], ['5.1', '3.8', '1.6', '0.2', 'Iris-setosa'], ['4.6', '3.2', '1.4', '0.2', 'Iris-setosa'], ['5.3', '3.7', '1.5', '0.2', 'Iris-setosa'], ['5', '3.3', '1.4', '0.2', 'Iris-setosa'], ['7', '3.2', '4.7', '1.4', 'Iris-versicolor'], ['6.4', '3.2', '4.5', '1.5', 'Iris-versicolor'], ['6.9', '3.1', '4.9', '1.5', 'Iris-versicolor'], ['5.5', '2.3', '4', '1.3', 'Iris-versicolor'], ['6.5', '2.8', '4.6', '1.5', 'Iris-versicolor'], ['5.7', '2.8', '4.5', '1.3', 'Iris-versicolor'], ['6.3', '3.3', '4.7', '1.6', 'Iris-versicolor'], ['4.9', '2.4', '3.3', '1', 'Iris-versicolor'], ['6.6', '2.9', '4.6', '1.3', 'Iris-versicolor'], ['5.2', '2.7', '3.9', '1.4', 'Iris-versicolor'], ['5', '2', '3.5', '1', 'Iris-versicolor'], ['5.9', '3', '4.2', '1.5', 'Iris-versicolor'], ['6', '2.2', '4', '1', 'Iris-versicolor'], ['6.1', '2.9', '4.7', '1.4', 'Iris-versicolor'], ['5.6', '2.9', '3.6', '1.3', 'Iris-versicolor'], ['6.7', '3.1', '4.4', '1.4', 'Iris-versicolor'], ['5.6', '3', '4.5', '1.5', 'Iris-versicolor'], ['5.8', '2.7', '4.1', '1', 'Iris-versicolor'], ['6.2', '2.2', '4.5', '1.5', 'Iris-versicolor'], ['5.6', '2.5', '3.9', '1.1', 'Iris-versicolor'], ['5.9', '3.2', '4.8', '1.8', 'Iris-versicolor'], ['6.1', '2.8', '4', '1.3', 'Iris-versicolor'], ['6.3', '2.5', '4.9', '1.5', 'Iris-versicolor'], ['6.1', '2.8', '4.7', '1.2', 'Iris-versicolor'], ['6.4', '2.9', '4.3', '1.3', 'Iris-versicolor'], ['6.6', '3', '4.4', '1.4', 'Iris-versicolor'], ['6.8', '2.8', '4.8', '1.4', 'Iris-versicolor'], ['6.7', '3', '5', '1.7', 'Iris-versicolor'], ['6', '2.9', '4.5', '1.5', 'Iris-versicolor'], ['5.7', '2.6', '3.5', '1', 'Iris-versicolor'], ['5.5', '2.4', '3.8', '1.1', 'Iris-versicolor'], ['5.5', '2.4', '3.7', '1', 'Iris-versicolor'], ['5.8', '2.7', '3.9', '1.2', 'Iris-versicolor'], ['6', '2.7', '5.1', '1.6', 'Iris-versicolor'], ['5.4', '3', '4.5', '1.5', 'Iris-versicolor'], ['6', '3.4', '4.5', '1.6', 'Iris-versicolor'], ['6.7', '3.1', '4.7', '1.5', 'Iris-versicolor'], ['6.3', '2.3', '4.4', '1.3', 'Iris-versicolor'], ['5.6', '3', '4.1', '1.3', 'Iris-versicolor'], ['5.5', '2.5', '4', '1.3', 'Iris-versicolor'], ['5.5', '2.6', '4.4', '1.2', 'Iris-versicolor'], ['6.1', '3', '4.6', '1.4', 'Iris-versicolor'], ['5.8', '2.6', '4', '1.2', 'Iris-versicolor'], ['5', '2.3', '3.3', '1', 'Iris-versicolor'], ['5.6', '2.7', '4.2', '1.3', 'Iris-versicolor'], ['5.7', '3', '4.2', '1.2', 'Iris-versicolor'], ['5.7', '2.9', '4.2', '1.3', 'Iris-versicolor'], ['6.2', '2.9', '4.3', '1.3', 'Iris-versicolor'], ['5.1', '2.5', '3', '1.1', 'Iris-versicolor'], ['5.7', '2.8', '4.1', '1.3', 'Iris-versicolor'], ['6.3', '3.3', '6', '2.5', 'Iris-virginica'], ['5.8', '2.7', '5.1', '1.9', 'Iris-virginica'], ['7.1', '3', '5.9', '2.1', 'Iris-virginica'], ['6.3', '2.9', '5.6', '1.8', 'Iris-virginica'], ['6.5', '3', '5.8', '2.2', 'Iris-virginica'], ['7.6', '3', '6.6', '2.1', 'Iris-virginica'], ['4.9', '2.5', '4.5', '1.7', 'Iris-virginica'], ['7.3', '2.9', '6.3', '1.8', 'Iris-virginica'], ['6.7', '2.5', '5.8', '1.8', 'Iris-virginica'], ['7.2', '3.6', '6.1', '2.5', 'Iris-virginica'], ['6.5', '3.2', '5.1', '2', 'Iris-virginica'], ['6.4', '2.7', '5.3', '1.9', 'Iris-virginica'], ['6.8', '3', '5.5', '2.1', 'Iris-virginica'], ['5.7', '2.5', '5', '2', 'Iris-virginica'], ['5.8', '2.8', '5.1', '2.4', 'Iris-virginica'], ['6.4', '3.2', '5.3', '2.3', 'Iris-virginica'], ['6.5', '3', '5.5', '1.8', 'Iris-virginica'], ['7.7', '3.8', '6.7', '2.2', 'Iris-virginica'], ['7.7', '2.6', '6.9', '2.3', 'Iris-virginica'], ['6', '2.2', '5', '1.5', 'Iris-virginica'], ['6.9', '3.2', '5.7', '2.3', 'Iris-virginica'], ['5.6', '2.8', '4.9', '2', 'Iris-virginica'], ['7.7', '2.8', '6.7', '2', 'Iris-virginica'], ['6.3', '2.7', '4.9', '1.8', 'Iris-virginica'], ['6.7', '3.3', '5.7', '2.1', 'Iris-virginica'], ['7.2', '3.2', '6', '1.8', 'Iris-virginica'], ['6.2', '2.8', '4.8', '1.8', 'Iris-virginica'], ['6.1', '3', '4.9', '1.8', 'Iris-virginica'], ['6.4', '2.8', '5.6', '2.1', 'Iris-virginica'], ['7.2', '3', '5.8', '1.6', 'Iris-virginica'], ['7.4', '2.8', '6.1', '1.9', 'Iris-virginica'], ['7.9', '3.8', '6.4', '2', 'Iris-virginica'], ['6.4', '2.8', '5.6', '2.2', 'Iris-virginica'], ['6.3', '2.8', '5.1', '1.5', 'Iris-virginica'], ['6.1', '2.6', '5.6', '1.4', 'Iris-virginica'], ['7.7', '3', '6.1', '2.3', 'Iris-virginica'], ['6.3', '3.4', '5.6', '2.4', 'Iris-virginica'], ['6.4', '3.1', '5.5', '1.8', 'Iris-virginica'], ['6', '3', '4.8', '1.8', 'Iris-virginica'], ['6.9', '3.1', '5.4', '2.1', 'Iris-virginica'], ['6.7', '3.1', '5.6', '2.4', 'Iris-virginica'], ['6.9', '3.1', '5.1', '2.3', 'Iris-virginica'], ['5.8', '2.7', '5.1', '1.9', 'Iris-virginica'], ['6.8', '3.2', '5.9', '2.3', 'Iris-virginica'], ['6.7', '3.3', '5.7', '2.5', 'Iris-virginica'], ['6.7', '3', '5.2', '2.3', 'Iris-virginica'], ['6.3', '2.5', '5', '1.9', 'Iris-virginica'], ['6.5', '3', '5.2', '2', 'Iris-virginica'], ['6.2', '3.4', '5.4', '2.3', 'Iris-virginica'], ['5.9', '3', '5.1', '1.8', 'Iris-virginica']]\n",
            "RAW DATA AS NUMPY\n",
            "[['sepal_length' 'sepal_width' 'petal_length' 'petal_width' 'species']\n",
            " ['5.1' '3.5' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.9' '3' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.7' '3.2' '1.3' '0.2' 'Iris-setosa']\n",
            " ['4.6' '3.1' '1.5' '0.2' 'Iris-setosa']\n",
            " ['5' '3.6' '1.4' '0.2' 'Iris-setosa']\n",
            " ['5.4' '3.9' '1.7' '0.4' 'Iris-setosa']\n",
            " ['4.6' '3.4' '1.4' '0.3' 'Iris-setosa']\n",
            " ['5' '3.4' '1.5' '0.2' 'Iris-setosa']\n",
            " ['4.4' '2.9' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.9' '3.1' '1.5' '0.1' 'Iris-setosa']\n",
            " ['5.4' '3.7' '1.5' '0.2' 'Iris-setosa']\n",
            " ['4.8' '3.4' '1.6' '0.2' 'Iris-setosa']\n",
            " ['4.8' '3' '1.4' '0.1' 'Iris-setosa']\n",
            " ['4.3' '3' '1.1' '0.1' 'Iris-setosa']\n",
            " ['5.8' '4' '1.2' '0.2' 'Iris-setosa']\n",
            " ['5.7' '4.4' '1.5' '0.4' 'Iris-setosa']\n",
            " ['5.4' '3.9' '1.3' '0.4' 'Iris-setosa']\n",
            " ['5.1' '3.5' '1.4' '0.3' 'Iris-setosa']\n",
            " ['5.7' '3.8' '1.7' '0.3' 'Iris-setosa']\n",
            " ['5.1' '3.8' '1.5' '0.3' 'Iris-setosa']\n",
            " ['5.4' '3.4' '1.7' '0.2' 'Iris-setosa']\n",
            " ['5.1' '3.7' '1.5' '0.4' 'Iris-setosa']\n",
            " ['4.6' '3.6' '1' '0.2' 'Iris-setosa']\n",
            " ['5.1' '3.3' '1.7' '0.5' 'Iris-setosa']\n",
            " ['4.8' '3.4' '1.9' '0.2' 'Iris-setosa']\n",
            " ['5' '3' '1.6' '0.2' 'Iris-setosa']\n",
            " ['5' '3.4' '1.6' '0.4' 'Iris-setosa']\n",
            " ['5.2' '3.5' '1.5' '0.2' 'Iris-setosa']\n",
            " ['5.2' '3.4' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.7' '3.2' '1.6' '0.2' 'Iris-setosa']\n",
            " ['4.8' '3.1' '1.6' '0.2' 'Iris-setosa']\n",
            " ['5.4' '3.4' '1.5' '0.4' 'Iris-setosa']\n",
            " ['5.2' '4.1' '1.5' '0.1' 'Iris-setosa']\n",
            " ['5.5' '4.2' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.9' '3.1' '1.5' '0.1' 'Iris-setosa']\n",
            " ['5' '3.2' '1.2' '0.2' 'Iris-setosa']\n",
            " ['5.5' '3.5' '1.3' '0.2' 'Iris-setosa']\n",
            " ['4.9' '3.1' '1.5' '0.1' 'Iris-setosa']\n",
            " ['4.4' '3' '1.3' '0.2' 'Iris-setosa']\n",
            " ['5.1' '3.4' '1.5' '0.2' 'Iris-setosa']\n",
            " ['5' '3.5' '1.3' '0.3' 'Iris-setosa']\n",
            " ['4.5' '2.3' '1.3' '0.3' 'Iris-setosa']\n",
            " ['4.4' '3.2' '1.3' '0.2' 'Iris-setosa']\n",
            " ['5' '3.5' '1.6' '0.6' 'Iris-setosa']\n",
            " ['5.1' '3.8' '1.9' '0.4' 'Iris-setosa']\n",
            " ['4.8' '3' '1.4' '0.3' 'Iris-setosa']\n",
            " ['5.1' '3.8' '1.6' '0.2' 'Iris-setosa']\n",
            " ['4.6' '3.2' '1.4' '0.2' 'Iris-setosa']\n",
            " ['5.3' '3.7' '1.5' '0.2' 'Iris-setosa']\n",
            " ['5' '3.3' '1.4' '0.2' 'Iris-setosa']\n",
            " ['7' '3.2' '4.7' '1.4' 'Iris-versicolor']\n",
            " ['6.4' '3.2' '4.5' '1.5' 'Iris-versicolor']\n",
            " ['6.9' '3.1' '4.9' '1.5' 'Iris-versicolor']\n",
            " ['5.5' '2.3' '4' '1.3' 'Iris-versicolor']\n",
            " ['6.5' '2.8' '4.6' '1.5' 'Iris-versicolor']\n",
            " ['5.7' '2.8' '4.5' '1.3' 'Iris-versicolor']\n",
            " ['6.3' '3.3' '4.7' '1.6' 'Iris-versicolor']\n",
            " ['4.9' '2.4' '3.3' '1' 'Iris-versicolor']\n",
            " ['6.6' '2.9' '4.6' '1.3' 'Iris-versicolor']\n",
            " ['5.2' '2.7' '3.9' '1.4' 'Iris-versicolor']\n",
            " ['5' '2' '3.5' '1' 'Iris-versicolor']\n",
            " ['5.9' '3' '4.2' '1.5' 'Iris-versicolor']\n",
            " ['6' '2.2' '4' '1' 'Iris-versicolor']\n",
            " ['6.1' '2.9' '4.7' '1.4' 'Iris-versicolor']\n",
            " ['5.6' '2.9' '3.6' '1.3' 'Iris-versicolor']\n",
            " ['6.7' '3.1' '4.4' '1.4' 'Iris-versicolor']\n",
            " ['5.6' '3' '4.5' '1.5' 'Iris-versicolor']\n",
            " ['5.8' '2.7' '4.1' '1' 'Iris-versicolor']\n",
            " ['6.2' '2.2' '4.5' '1.5' 'Iris-versicolor']\n",
            " ['5.6' '2.5' '3.9' '1.1' 'Iris-versicolor']\n",
            " ['5.9' '3.2' '4.8' '1.8' 'Iris-versicolor']\n",
            " ['6.1' '2.8' '4' '1.3' 'Iris-versicolor']\n",
            " ['6.3' '2.5' '4.9' '1.5' 'Iris-versicolor']\n",
            " ['6.1' '2.8' '4.7' '1.2' 'Iris-versicolor']\n",
            " ['6.4' '2.9' '4.3' '1.3' 'Iris-versicolor']\n",
            " ['6.6' '3' '4.4' '1.4' 'Iris-versicolor']\n",
            " ['6.8' '2.8' '4.8' '1.4' 'Iris-versicolor']\n",
            " ['6.7' '3' '5' '1.7' 'Iris-versicolor']\n",
            " ['6' '2.9' '4.5' '1.5' 'Iris-versicolor']\n",
            " ['5.7' '2.6' '3.5' '1' 'Iris-versicolor']\n",
            " ['5.5' '2.4' '3.8' '1.1' 'Iris-versicolor']\n",
            " ['5.5' '2.4' '3.7' '1' 'Iris-versicolor']\n",
            " ['5.8' '2.7' '3.9' '1.2' 'Iris-versicolor']\n",
            " ['6' '2.7' '5.1' '1.6' 'Iris-versicolor']\n",
            " ['5.4' '3' '4.5' '1.5' 'Iris-versicolor']\n",
            " ['6' '3.4' '4.5' '1.6' 'Iris-versicolor']\n",
            " ['6.7' '3.1' '4.7' '1.5' 'Iris-versicolor']\n",
            " ['6.3' '2.3' '4.4' '1.3' 'Iris-versicolor']\n",
            " ['5.6' '3' '4.1' '1.3' 'Iris-versicolor']\n",
            " ['5.5' '2.5' '4' '1.3' 'Iris-versicolor']\n",
            " ['5.5' '2.6' '4.4' '1.2' 'Iris-versicolor']\n",
            " ['6.1' '3' '4.6' '1.4' 'Iris-versicolor']\n",
            " ['5.8' '2.6' '4' '1.2' 'Iris-versicolor']\n",
            " ['5' '2.3' '3.3' '1' 'Iris-versicolor']\n",
            " ['5.6' '2.7' '4.2' '1.3' 'Iris-versicolor']\n",
            " ['5.7' '3' '4.2' '1.2' 'Iris-versicolor']\n",
            " ['5.7' '2.9' '4.2' '1.3' 'Iris-versicolor']\n",
            " ['6.2' '2.9' '4.3' '1.3' 'Iris-versicolor']\n",
            " ['5.1' '2.5' '3' '1.1' 'Iris-versicolor']\n",
            " ['5.7' '2.8' '4.1' '1.3' 'Iris-versicolor']\n",
            " ['6.3' '3.3' '6' '2.5' 'Iris-virginica']\n",
            " ['5.8' '2.7' '5.1' '1.9' 'Iris-virginica']\n",
            " ['7.1' '3' '5.9' '2.1' 'Iris-virginica']\n",
            " ['6.3' '2.9' '5.6' '1.8' 'Iris-virginica']\n",
            " ['6.5' '3' '5.8' '2.2' 'Iris-virginica']\n",
            " ['7.6' '3' '6.6' '2.1' 'Iris-virginica']\n",
            " ['4.9' '2.5' '4.5' '1.7' 'Iris-virginica']\n",
            " ['7.3' '2.9' '6.3' '1.8' 'Iris-virginica']\n",
            " ['6.7' '2.5' '5.8' '1.8' 'Iris-virginica']\n",
            " ['7.2' '3.6' '6.1' '2.5' 'Iris-virginica']\n",
            " ['6.5' '3.2' '5.1' '2' 'Iris-virginica']\n",
            " ['6.4' '2.7' '5.3' '1.9' 'Iris-virginica']\n",
            " ['6.8' '3' '5.5' '2.1' 'Iris-virginica']\n",
            " ['5.7' '2.5' '5' '2' 'Iris-virginica']\n",
            " ['5.8' '2.8' '5.1' '2.4' 'Iris-virginica']\n",
            " ['6.4' '3.2' '5.3' '2.3' 'Iris-virginica']\n",
            " ['6.5' '3' '5.5' '1.8' 'Iris-virginica']\n",
            " ['7.7' '3.8' '6.7' '2.2' 'Iris-virginica']\n",
            " ['7.7' '2.6' '6.9' '2.3' 'Iris-virginica']\n",
            " ['6' '2.2' '5' '1.5' 'Iris-virginica']\n",
            " ['6.9' '3.2' '5.7' '2.3' 'Iris-virginica']\n",
            " ['5.6' '2.8' '4.9' '2' 'Iris-virginica']\n",
            " ['7.7' '2.8' '6.7' '2' 'Iris-virginica']\n",
            " ['6.3' '2.7' '4.9' '1.8' 'Iris-virginica']\n",
            " ['6.7' '3.3' '5.7' '2.1' 'Iris-virginica']\n",
            " ['7.2' '3.2' '6' '1.8' 'Iris-virginica']\n",
            " ['6.2' '2.8' '4.8' '1.8' 'Iris-virginica']\n",
            " ['6.1' '3' '4.9' '1.8' 'Iris-virginica']\n",
            " ['6.4' '2.8' '5.6' '2.1' 'Iris-virginica']\n",
            " ['7.2' '3' '5.8' '1.6' 'Iris-virginica']\n",
            " ['7.4' '2.8' '6.1' '1.9' 'Iris-virginica']\n",
            " ['7.9' '3.8' '6.4' '2' 'Iris-virginica']\n",
            " ['6.4' '2.8' '5.6' '2.2' 'Iris-virginica']\n",
            " ['6.3' '2.8' '5.1' '1.5' 'Iris-virginica']\n",
            " ['6.1' '2.6' '5.6' '1.4' 'Iris-virginica']\n",
            " ['7.7' '3' '6.1' '2.3' 'Iris-virginica']\n",
            " ['6.3' '3.4' '5.6' '2.4' 'Iris-virginica']\n",
            " ['6.4' '3.1' '5.5' '1.8' 'Iris-virginica']\n",
            " ['6' '3' '4.8' '1.8' 'Iris-virginica']\n",
            " ['6.9' '3.1' '5.4' '2.1' 'Iris-virginica']\n",
            " ['6.7' '3.1' '5.6' '2.4' 'Iris-virginica']\n",
            " ['6.9' '3.1' '5.1' '2.3' 'Iris-virginica']\n",
            " ['5.8' '2.7' '5.1' '1.9' 'Iris-virginica']\n",
            " ['6.8' '3.2' '5.9' '2.3' 'Iris-virginica']\n",
            " ['6.7' '3.3' '5.7' '2.5' 'Iris-virginica']\n",
            " ['6.7' '3' '5.2' '2.3' 'Iris-virginica']\n",
            " ['6.3' '2.5' '5' '1.9' 'Iris-virginica']\n",
            " ['6.5' '3' '5.2' '2' 'Iris-virginica']\n",
            " ['6.2' '3.4' '5.4' '2.3' 'Iris-virginica']\n",
            " ['5.9' '3' '5.1' '1.8' 'Iris-virginica']]\n",
            "SEPARATED X AND Y\n",
            "X\n",
            "[['5.1' '3.5' '1.4' '0.2']\n",
            " ['4.9' '3' '1.4' '0.2']\n",
            " ['4.7' '3.2' '1.3' '0.2']\n",
            " ['4.6' '3.1' '1.5' '0.2']\n",
            " ['5' '3.6' '1.4' '0.2']\n",
            " ['5.4' '3.9' '1.7' '0.4']\n",
            " ['4.6' '3.4' '1.4' '0.3']\n",
            " ['5' '3.4' '1.5' '0.2']\n",
            " ['4.4' '2.9' '1.4' '0.2']\n",
            " ['4.9' '3.1' '1.5' '0.1']\n",
            " ['5.4' '3.7' '1.5' '0.2']\n",
            " ['4.8' '3.4' '1.6' '0.2']\n",
            " ['4.8' '3' '1.4' '0.1']\n",
            " ['4.3' '3' '1.1' '0.1']\n",
            " ['5.8' '4' '1.2' '0.2']\n",
            " ['5.7' '4.4' '1.5' '0.4']\n",
            " ['5.4' '3.9' '1.3' '0.4']\n",
            " ['5.1' '3.5' '1.4' '0.3']\n",
            " ['5.7' '3.8' '1.7' '0.3']\n",
            " ['5.1' '3.8' '1.5' '0.3']\n",
            " ['5.4' '3.4' '1.7' '0.2']\n",
            " ['5.1' '3.7' '1.5' '0.4']\n",
            " ['4.6' '3.6' '1' '0.2']\n",
            " ['5.1' '3.3' '1.7' '0.5']\n",
            " ['4.8' '3.4' '1.9' '0.2']\n",
            " ['5' '3' '1.6' '0.2']\n",
            " ['5' '3.4' '1.6' '0.4']\n",
            " ['5.2' '3.5' '1.5' '0.2']\n",
            " ['5.2' '3.4' '1.4' '0.2']\n",
            " ['4.7' '3.2' '1.6' '0.2']\n",
            " ['4.8' '3.1' '1.6' '0.2']\n",
            " ['5.4' '3.4' '1.5' '0.4']\n",
            " ['5.2' '4.1' '1.5' '0.1']\n",
            " ['5.5' '4.2' '1.4' '0.2']\n",
            " ['4.9' '3.1' '1.5' '0.1']\n",
            " ['5' '3.2' '1.2' '0.2']\n",
            " ['5.5' '3.5' '1.3' '0.2']\n",
            " ['4.9' '3.1' '1.5' '0.1']\n",
            " ['4.4' '3' '1.3' '0.2']\n",
            " ['5.1' '3.4' '1.5' '0.2']\n",
            " ['5' '3.5' '1.3' '0.3']\n",
            " ['4.5' '2.3' '1.3' '0.3']\n",
            " ['4.4' '3.2' '1.3' '0.2']\n",
            " ['5' '3.5' '1.6' '0.6']\n",
            " ['5.1' '3.8' '1.9' '0.4']\n",
            " ['4.8' '3' '1.4' '0.3']\n",
            " ['5.1' '3.8' '1.6' '0.2']\n",
            " ['4.6' '3.2' '1.4' '0.2']\n",
            " ['5.3' '3.7' '1.5' '0.2']\n",
            " ['5' '3.3' '1.4' '0.2']\n",
            " ['7' '3.2' '4.7' '1.4']\n",
            " ['6.4' '3.2' '4.5' '1.5']\n",
            " ['6.9' '3.1' '4.9' '1.5']\n",
            " ['5.5' '2.3' '4' '1.3']\n",
            " ['6.5' '2.8' '4.6' '1.5']\n",
            " ['5.7' '2.8' '4.5' '1.3']\n",
            " ['6.3' '3.3' '4.7' '1.6']\n",
            " ['4.9' '2.4' '3.3' '1']\n",
            " ['6.6' '2.9' '4.6' '1.3']\n",
            " ['5.2' '2.7' '3.9' '1.4']\n",
            " ['5' '2' '3.5' '1']\n",
            " ['5.9' '3' '4.2' '1.5']\n",
            " ['6' '2.2' '4' '1']\n",
            " ['6.1' '2.9' '4.7' '1.4']\n",
            " ['5.6' '2.9' '3.6' '1.3']\n",
            " ['6.7' '3.1' '4.4' '1.4']\n",
            " ['5.6' '3' '4.5' '1.5']\n",
            " ['5.8' '2.7' '4.1' '1']\n",
            " ['6.2' '2.2' '4.5' '1.5']\n",
            " ['5.6' '2.5' '3.9' '1.1']\n",
            " ['5.9' '3.2' '4.8' '1.8']\n",
            " ['6.1' '2.8' '4' '1.3']\n",
            " ['6.3' '2.5' '4.9' '1.5']\n",
            " ['6.1' '2.8' '4.7' '1.2']\n",
            " ['6.4' '2.9' '4.3' '1.3']\n",
            " ['6.6' '3' '4.4' '1.4']\n",
            " ['6.8' '2.8' '4.8' '1.4']\n",
            " ['6.7' '3' '5' '1.7']\n",
            " ['6' '2.9' '4.5' '1.5']\n",
            " ['5.7' '2.6' '3.5' '1']\n",
            " ['5.5' '2.4' '3.8' '1.1']\n",
            " ['5.5' '2.4' '3.7' '1']\n",
            " ['5.8' '2.7' '3.9' '1.2']\n",
            " ['6' '2.7' '5.1' '1.6']\n",
            " ['5.4' '3' '4.5' '1.5']\n",
            " ['6' '3.4' '4.5' '1.6']\n",
            " ['6.7' '3.1' '4.7' '1.5']\n",
            " ['6.3' '2.3' '4.4' '1.3']\n",
            " ['5.6' '3' '4.1' '1.3']\n",
            " ['5.5' '2.5' '4' '1.3']\n",
            " ['5.5' '2.6' '4.4' '1.2']\n",
            " ['6.1' '3' '4.6' '1.4']\n",
            " ['5.8' '2.6' '4' '1.2']\n",
            " ['5' '2.3' '3.3' '1']\n",
            " ['5.6' '2.7' '4.2' '1.3']\n",
            " ['5.7' '3' '4.2' '1.2']\n",
            " ['5.7' '2.9' '4.2' '1.3']\n",
            " ['6.2' '2.9' '4.3' '1.3']\n",
            " ['5.1' '2.5' '3' '1.1']\n",
            " ['5.7' '2.8' '4.1' '1.3']\n",
            " ['6.3' '3.3' '6' '2.5']\n",
            " ['5.8' '2.7' '5.1' '1.9']\n",
            " ['7.1' '3' '5.9' '2.1']\n",
            " ['6.3' '2.9' '5.6' '1.8']\n",
            " ['6.5' '3' '5.8' '2.2']\n",
            " ['7.6' '3' '6.6' '2.1']\n",
            " ['4.9' '2.5' '4.5' '1.7']\n",
            " ['7.3' '2.9' '6.3' '1.8']\n",
            " ['6.7' '2.5' '5.8' '1.8']\n",
            " ['7.2' '3.6' '6.1' '2.5']\n",
            " ['6.5' '3.2' '5.1' '2']\n",
            " ['6.4' '2.7' '5.3' '1.9']\n",
            " ['6.8' '3' '5.5' '2.1']\n",
            " ['5.7' '2.5' '5' '2']\n",
            " ['5.8' '2.8' '5.1' '2.4']\n",
            " ['6.4' '3.2' '5.3' '2.3']\n",
            " ['6.5' '3' '5.5' '1.8']\n",
            " ['7.7' '3.8' '6.7' '2.2']\n",
            " ['7.7' '2.6' '6.9' '2.3']\n",
            " ['6' '2.2' '5' '1.5']\n",
            " ['6.9' '3.2' '5.7' '2.3']\n",
            " ['5.6' '2.8' '4.9' '2']\n",
            " ['7.7' '2.8' '6.7' '2']\n",
            " ['6.3' '2.7' '4.9' '1.8']\n",
            " ['6.7' '3.3' '5.7' '2.1']\n",
            " ['7.2' '3.2' '6' '1.8']\n",
            " ['6.2' '2.8' '4.8' '1.8']\n",
            " ['6.1' '3' '4.9' '1.8']\n",
            " ['6.4' '2.8' '5.6' '2.1']\n",
            " ['7.2' '3' '5.8' '1.6']\n",
            " ['7.4' '2.8' '6.1' '1.9']\n",
            " ['7.9' '3.8' '6.4' '2']\n",
            " ['6.4' '2.8' '5.6' '2.2']\n",
            " ['6.3' '2.8' '5.1' '1.5']\n",
            " ['6.1' '2.6' '5.6' '1.4']\n",
            " ['7.7' '3' '6.1' '2.3']\n",
            " ['6.3' '3.4' '5.6' '2.4']\n",
            " ['6.4' '3.1' '5.5' '1.8']\n",
            " ['6' '3' '4.8' '1.8']\n",
            " ['6.9' '3.1' '5.4' '2.1']\n",
            " ['6.7' '3.1' '5.6' '2.4']\n",
            " ['6.9' '3.1' '5.1' '2.3']\n",
            " ['5.8' '2.7' '5.1' '1.9']\n",
            " ['6.8' '3.2' '5.9' '2.3']\n",
            " ['6.7' '3.3' '5.7' '2.5']\n",
            " ['6.7' '3' '5.2' '2.3']\n",
            " ['6.3' '2.5' '5' '1.9']\n",
            " ['6.5' '3' '5.2' '2']\n",
            " ['6.2' '3.4' '5.4' '2.3']\n",
            " ['5.9' '3' '5.1' '1.8']]\n",
            "Y\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n",
            "CONVERTED X TO FLOATS\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "CONVERTED CLASSES NAMES TO IDS\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "[(0, 'Iris-setosa'), (1, 'Iris-versicolor'), (2, 'Iris-virginica')]\n",
            "BIAS TRICK\n",
            "[[5.1 3.5 1.4 0.2 1. ]\n",
            " [4.9 3.  1.4 0.2 1. ]\n",
            " [4.7 3.2 1.3 0.2 1. ]\n",
            " [4.6 3.1 1.5 0.2 1. ]\n",
            " [5.  3.6 1.4 0.2 1. ]\n",
            " [5.4 3.9 1.7 0.4 1. ]\n",
            " [4.6 3.4 1.4 0.3 1. ]\n",
            " [5.  3.4 1.5 0.2 1. ]\n",
            " [4.4 2.9 1.4 0.2 1. ]\n",
            " [4.9 3.1 1.5 0.1 1. ]\n",
            " [5.4 3.7 1.5 0.2 1. ]\n",
            " [4.8 3.4 1.6 0.2 1. ]\n",
            " [4.8 3.  1.4 0.1 1. ]\n",
            " [4.3 3.  1.1 0.1 1. ]\n",
            " [5.8 4.  1.2 0.2 1. ]\n",
            " [5.7 4.4 1.5 0.4 1. ]\n",
            " [5.4 3.9 1.3 0.4 1. ]\n",
            " [5.1 3.5 1.4 0.3 1. ]\n",
            " [5.7 3.8 1.7 0.3 1. ]\n",
            " [5.1 3.8 1.5 0.3 1. ]\n",
            " [5.4 3.4 1.7 0.2 1. ]\n",
            " [5.1 3.7 1.5 0.4 1. ]\n",
            " [4.6 3.6 1.  0.2 1. ]\n",
            " [5.1 3.3 1.7 0.5 1. ]\n",
            " [4.8 3.4 1.9 0.2 1. ]\n",
            " [5.  3.  1.6 0.2 1. ]\n",
            " [5.  3.4 1.6 0.4 1. ]\n",
            " [5.2 3.5 1.5 0.2 1. ]\n",
            " [5.2 3.4 1.4 0.2 1. ]\n",
            " [4.7 3.2 1.6 0.2 1. ]\n",
            " [4.8 3.1 1.6 0.2 1. ]\n",
            " [5.4 3.4 1.5 0.4 1. ]\n",
            " [5.2 4.1 1.5 0.1 1. ]\n",
            " [5.5 4.2 1.4 0.2 1. ]\n",
            " [4.9 3.1 1.5 0.1 1. ]\n",
            " [5.  3.2 1.2 0.2 1. ]\n",
            " [5.5 3.5 1.3 0.2 1. ]\n",
            " [4.9 3.1 1.5 0.1 1. ]\n",
            " [4.4 3.  1.3 0.2 1. ]\n",
            " [5.1 3.4 1.5 0.2 1. ]\n",
            " [5.  3.5 1.3 0.3 1. ]\n",
            " [4.5 2.3 1.3 0.3 1. ]\n",
            " [4.4 3.2 1.3 0.2 1. ]\n",
            " [5.  3.5 1.6 0.6 1. ]\n",
            " [5.1 3.8 1.9 0.4 1. ]\n",
            " [4.8 3.  1.4 0.3 1. ]\n",
            " [5.1 3.8 1.6 0.2 1. ]\n",
            " [4.6 3.2 1.4 0.2 1. ]\n",
            " [5.3 3.7 1.5 0.2 1. ]\n",
            " [5.  3.3 1.4 0.2 1. ]\n",
            " [7.  3.2 4.7 1.4 1. ]\n",
            " [6.4 3.2 4.5 1.5 1. ]\n",
            " [6.9 3.1 4.9 1.5 1. ]\n",
            " [5.5 2.3 4.  1.3 1. ]\n",
            " [6.5 2.8 4.6 1.5 1. ]\n",
            " [5.7 2.8 4.5 1.3 1. ]\n",
            " [6.3 3.3 4.7 1.6 1. ]\n",
            " [4.9 2.4 3.3 1.  1. ]\n",
            " [6.6 2.9 4.6 1.3 1. ]\n",
            " [5.2 2.7 3.9 1.4 1. ]\n",
            " [5.  2.  3.5 1.  1. ]\n",
            " [5.9 3.  4.2 1.5 1. ]\n",
            " [6.  2.2 4.  1.  1. ]\n",
            " [6.1 2.9 4.7 1.4 1. ]\n",
            " [5.6 2.9 3.6 1.3 1. ]\n",
            " [6.7 3.1 4.4 1.4 1. ]\n",
            " [5.6 3.  4.5 1.5 1. ]\n",
            " [5.8 2.7 4.1 1.  1. ]\n",
            " [6.2 2.2 4.5 1.5 1. ]\n",
            " [5.6 2.5 3.9 1.1 1. ]\n",
            " [5.9 3.2 4.8 1.8 1. ]\n",
            " [6.1 2.8 4.  1.3 1. ]\n",
            " [6.3 2.5 4.9 1.5 1. ]\n",
            " [6.1 2.8 4.7 1.2 1. ]\n",
            " [6.4 2.9 4.3 1.3 1. ]\n",
            " [6.6 3.  4.4 1.4 1. ]\n",
            " [6.8 2.8 4.8 1.4 1. ]\n",
            " [6.7 3.  5.  1.7 1. ]\n",
            " [6.  2.9 4.5 1.5 1. ]\n",
            " [5.7 2.6 3.5 1.  1. ]\n",
            " [5.5 2.4 3.8 1.1 1. ]\n",
            " [5.5 2.4 3.7 1.  1. ]\n",
            " [5.8 2.7 3.9 1.2 1. ]\n",
            " [6.  2.7 5.1 1.6 1. ]\n",
            " [5.4 3.  4.5 1.5 1. ]\n",
            " [6.  3.4 4.5 1.6 1. ]\n",
            " [6.7 3.1 4.7 1.5 1. ]\n",
            " [6.3 2.3 4.4 1.3 1. ]\n",
            " [5.6 3.  4.1 1.3 1. ]\n",
            " [5.5 2.5 4.  1.3 1. ]\n",
            " [5.5 2.6 4.4 1.2 1. ]\n",
            " [6.1 3.  4.6 1.4 1. ]\n",
            " [5.8 2.6 4.  1.2 1. ]\n",
            " [5.  2.3 3.3 1.  1. ]\n",
            " [5.6 2.7 4.2 1.3 1. ]\n",
            " [5.7 3.  4.2 1.2 1. ]\n",
            " [5.7 2.9 4.2 1.3 1. ]\n",
            " [6.2 2.9 4.3 1.3 1. ]\n",
            " [5.1 2.5 3.  1.1 1. ]\n",
            " [5.7 2.8 4.1 1.3 1. ]\n",
            " [6.3 3.3 6.  2.5 1. ]\n",
            " [5.8 2.7 5.1 1.9 1. ]\n",
            " [7.1 3.  5.9 2.1 1. ]\n",
            " [6.3 2.9 5.6 1.8 1. ]\n",
            " [6.5 3.  5.8 2.2 1. ]\n",
            " [7.6 3.  6.6 2.1 1. ]\n",
            " [4.9 2.5 4.5 1.7 1. ]\n",
            " [7.3 2.9 6.3 1.8 1. ]\n",
            " [6.7 2.5 5.8 1.8 1. ]\n",
            " [7.2 3.6 6.1 2.5 1. ]\n",
            " [6.5 3.2 5.1 2.  1. ]\n",
            " [6.4 2.7 5.3 1.9 1. ]\n",
            " [6.8 3.  5.5 2.1 1. ]\n",
            " [5.7 2.5 5.  2.  1. ]\n",
            " [5.8 2.8 5.1 2.4 1. ]\n",
            " [6.4 3.2 5.3 2.3 1. ]\n",
            " [6.5 3.  5.5 1.8 1. ]\n",
            " [7.7 3.8 6.7 2.2 1. ]\n",
            " [7.7 2.6 6.9 2.3 1. ]\n",
            " [6.  2.2 5.  1.5 1. ]\n",
            " [6.9 3.2 5.7 2.3 1. ]\n",
            " [5.6 2.8 4.9 2.  1. ]\n",
            " [7.7 2.8 6.7 2.  1. ]\n",
            " [6.3 2.7 4.9 1.8 1. ]\n",
            " [6.7 3.3 5.7 2.1 1. ]\n",
            " [7.2 3.2 6.  1.8 1. ]\n",
            " [6.2 2.8 4.8 1.8 1. ]\n",
            " [6.1 3.  4.9 1.8 1. ]\n",
            " [6.4 2.8 5.6 2.1 1. ]\n",
            " [7.2 3.  5.8 1.6 1. ]\n",
            " [7.4 2.8 6.1 1.9 1. ]\n",
            " [7.9 3.8 6.4 2.  1. ]\n",
            " [6.4 2.8 5.6 2.2 1. ]\n",
            " [6.3 2.8 5.1 1.5 1. ]\n",
            " [6.1 2.6 5.6 1.4 1. ]\n",
            " [7.7 3.  6.1 2.3 1. ]\n",
            " [6.3 3.4 5.6 2.4 1. ]\n",
            " [6.4 3.1 5.5 1.8 1. ]\n",
            " [6.  3.  4.8 1.8 1. ]\n",
            " [6.9 3.1 5.4 2.1 1. ]\n",
            " [6.7 3.1 5.6 2.4 1. ]\n",
            " [6.9 3.1 5.1 2.3 1. ]\n",
            " [5.8 2.7 5.1 1.9 1. ]\n",
            " [6.8 3.2 5.9 2.3 1. ]\n",
            " [6.7 3.3 5.7 2.5 1. ]\n",
            " [6.7 3.  5.2 2.3 1. ]\n",
            " [6.3 2.5 5.  1.9 1. ]\n",
            " [6.5 3.  5.2 2.  1. ]\n",
            " [6.2 3.4 5.4 2.3 1. ]\n",
            " [5.9 3.  5.1 1.8 1. ]]\n",
            "TRAINING AND TEST SETS\n",
            "TRAINING SET\n",
            "[[5.1 3.8 1.9 0.4 1. ]\n",
            " [4.6 3.4 1.4 0.3 1. ]\n",
            " [5.1 3.5 1.4 0.3 1. ]\n",
            " [4.7 3.2 1.3 0.2 1. ]\n",
            " [5.  3.5 1.6 0.6 1. ]\n",
            " [4.6 3.1 1.5 0.2 1. ]\n",
            " [5.3 3.7 1.5 0.2 1. ]\n",
            " [4.8 3.  1.4 0.3 1. ]\n",
            " [5.4 3.9 1.3 0.4 1. ]\n",
            " [5.4 3.4 1.7 0.2 1. ]\n",
            " [5.1 3.3 1.7 0.5 1. ]\n",
            " [4.9 3.1 1.5 0.1 1. ]\n",
            " [5.5 3.5 1.3 0.2 1. ]\n",
            " [4.4 3.  1.3 0.2 1. ]\n",
            " [4.8 3.  1.4 0.1 1. ]\n",
            " [5.  3.4 1.5 0.2 1. ]\n",
            " [5.2 3.5 1.5 0.2 1. ]\n",
            " [4.6 3.6 1.  0.2 1. ]\n",
            " [5.1 3.4 1.5 0.2 1. ]\n",
            " [5.7 4.4 1.5 0.4 1. ]\n",
            " [5.1 3.7 1.5 0.4 1. ]\n",
            " [5.2 4.1 1.5 0.1 1. ]\n",
            " [5.4 3.9 1.7 0.4 1. ]\n",
            " [5.8 4.  1.2 0.2 1. ]\n",
            " [4.8 3.1 1.6 0.2 1. ]\n",
            " [4.4 3.2 1.3 0.2 1. ]\n",
            " [4.3 3.  1.1 0.1 1. ]\n",
            " [5.  3.5 1.3 0.3 1. ]\n",
            " [5.7 3.8 1.7 0.3 1. ]\n",
            " [5.1 3.5 1.4 0.2 1. ]\n",
            " [4.8 3.4 1.6 0.2 1. ]\n",
            " [5.1 3.8 1.6 0.2 1. ]\n",
            " [4.6 3.2 1.4 0.2 1. ]\n",
            " [5.  3.6 1.4 0.2 1. ]\n",
            " [4.4 2.9 1.4 0.2 1. ]\n",
            " [5.4 3.7 1.5 0.2 1. ]\n",
            " [4.9 3.  1.4 0.2 1. ]\n",
            " [4.9 3.1 1.5 0.1 1. ]\n",
            " [4.9 3.1 1.5 0.1 1. ]\n",
            " [4.7 3.2 1.6 0.2 1. ]\n",
            " [5.5 2.4 3.8 1.1 1. ]\n",
            " [5.9 3.  4.2 1.5 1. ]\n",
            " [6.7 3.  5.  1.7 1. ]\n",
            " [5.6 2.5 3.9 1.1 1. ]\n",
            " [5.2 2.7 3.9 1.4 1. ]\n",
            " [6.7 3.1 4.7 1.5 1. ]\n",
            " [6.3 2.5 4.9 1.5 1. ]\n",
            " [7.  3.2 4.7 1.4 1. ]\n",
            " [5.4 3.  4.5 1.5 1. ]\n",
            " [5.6 3.  4.5 1.5 1. ]\n",
            " [6.7 3.1 4.4 1.4 1. ]\n",
            " [5.1 2.5 3.  1.1 1. ]\n",
            " [5.6 3.  4.1 1.3 1. ]\n",
            " [6.5 2.8 4.6 1.5 1. ]\n",
            " [5.8 2.7 4.1 1.  1. ]\n",
            " [6.9 3.1 4.9 1.5 1. ]\n",
            " [5.9 3.2 4.8 1.8 1. ]\n",
            " [6.1 2.8 4.7 1.2 1. ]\n",
            " [6.3 3.3 4.7 1.6 1. ]\n",
            " [4.9 2.4 3.3 1.  1. ]\n",
            " [5.8 2.6 4.  1.2 1. ]\n",
            " [5.5 2.5 4.  1.3 1. ]\n",
            " [6.  2.9 4.5 1.5 1. ]\n",
            " [6.3 2.3 4.4 1.3 1. ]\n",
            " [6.  2.7 5.1 1.6 1. ]\n",
            " [5.8 2.7 3.9 1.2 1. ]\n",
            " [5.7 2.6 3.5 1.  1. ]\n",
            " [6.2 2.9 4.3 1.3 1. ]\n",
            " [6.  3.4 4.5 1.6 1. ]\n",
            " [6.1 3.  4.6 1.4 1. ]\n",
            " [5.7 2.8 4.5 1.3 1. ]\n",
            " [5.7 2.9 4.2 1.3 1. ]\n",
            " [5.7 3.  4.2 1.2 1. ]\n",
            " [5.5 2.4 3.7 1.  1. ]\n",
            " [6.  2.2 4.  1.  1. ]\n",
            " [5.  2.3 3.3 1.  1. ]\n",
            " [6.2 2.2 4.5 1.5 1. ]\n",
            " [5.6 2.7 4.2 1.3 1. ]\n",
            " [5.6 2.9 3.6 1.3 1. ]\n",
            " [6.8 2.8 4.8 1.4 1. ]\n",
            " [7.1 3.  5.9 2.1 1. ]\n",
            " [6.3 2.5 5.  1.9 1. ]\n",
            " [6.5 3.2 5.1 2.  1. ]\n",
            " [7.7 2.8 6.7 2.  1. ]\n",
            " [6.3 3.3 6.  2.5 1. ]\n",
            " [6.3 3.4 5.6 2.4 1. ]\n",
            " [5.8 2.7 5.1 1.9 1. ]\n",
            " [6.8 3.  5.5 2.1 1. ]\n",
            " [7.6 3.  6.6 2.1 1. ]\n",
            " [6.7 3.3 5.7 2.1 1. ]\n",
            " [5.9 3.  5.1 1.8 1. ]\n",
            " [6.4 2.8 5.6 2.1 1. ]\n",
            " [6.5 3.  5.2 2.  1. ]\n",
            " [6.7 3.3 5.7 2.5 1. ]\n",
            " [6.3 2.9 5.6 1.8 1. ]\n",
            " [6.7 3.1 5.6 2.4 1. ]\n",
            " [5.6 2.8 4.9 2.  1. ]\n",
            " [6.2 2.8 4.8 1.8 1. ]\n",
            " [6.7 2.5 5.8 1.8 1. ]\n",
            " [7.9 3.8 6.4 2.  1. ]\n",
            " [6.9 3.1 5.1 2.3 1. ]\n",
            " [6.3 2.8 5.1 1.5 1. ]\n",
            " [6.4 3.2 5.3 2.3 1. ]\n",
            " [5.8 2.7 5.1 1.9 1. ]\n",
            " [7.3 2.9 6.3 1.8 1. ]\n",
            " [6.4 2.7 5.3 1.9 1. ]\n",
            " [6.5 3.  5.5 1.8 1. ]\n",
            " [6.  2.2 5.  1.5 1. ]\n",
            " [6.7 3.  5.2 2.3 1. ]\n",
            " [7.2 3.6 6.1 2.5 1. ]\n",
            " [6.3 2.7 4.9 1.8 1. ]\n",
            " [6.1 3.  4.9 1.8 1. ]\n",
            " [7.2 3.  5.8 1.6 1. ]\n",
            " [6.4 3.1 5.5 1.8 1. ]\n",
            " [5.8 2.8 5.1 2.4 1. ]\n",
            " [6.  3.  4.8 1.8 1. ]\n",
            " [6.8 3.2 5.9 2.3 1. ]\n",
            " [7.7 2.6 6.9 2.3 1. ]\n",
            " [6.9 3.1 5.4 2.1 1. ]\n",
            " [6.2 3.4 5.4 2.3 1. ]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2]\n",
            "TEST SET (20% OF THE DATASET)\n",
            "[[5.5 4.2 1.4 0.2 1. ]\n",
            " [5.  3.  1.6 0.2 1. ]\n",
            " [4.5 2.3 1.3 0.3 1. ]\n",
            " [5.2 3.4 1.4 0.2 1. ]\n",
            " [5.  3.2 1.2 0.2 1. ]\n",
            " [5.  3.4 1.6 0.4 1. ]\n",
            " [5.4 3.4 1.5 0.4 1. ]\n",
            " [4.8 3.4 1.9 0.2 1. ]\n",
            " [5.1 3.8 1.5 0.3 1. ]\n",
            " [5.  3.3 1.4 0.2 1. ]\n",
            " [6.4 2.9 4.3 1.3 1. ]\n",
            " [5.7 2.8 4.1 1.3 1. ]\n",
            " [5.5 2.3 4.  1.3 1. ]\n",
            " [6.1 2.8 4.  1.3 1. ]\n",
            " [5.5 2.6 4.4 1.2 1. ]\n",
            " [5.  2.  3.5 1.  1. ]\n",
            " [6.6 2.9 4.6 1.3 1. ]\n",
            " [6.1 2.9 4.7 1.4 1. ]\n",
            " [6.6 3.  4.4 1.4 1. ]\n",
            " [6.4 3.2 4.5 1.5 1. ]\n",
            " [6.5 3.  5.8 2.2 1. ]\n",
            " [5.7 2.5 5.  2.  1. ]\n",
            " [6.4 2.8 5.6 2.2 1. ]\n",
            " [7.4 2.8 6.1 1.9 1. ]\n",
            " [6.1 2.6 5.6 1.4 1. ]\n",
            " [7.7 3.  6.1 2.3 1. ]\n",
            " [4.9 2.5 4.5 1.7 1. ]\n",
            " [7.2 3.2 6.  1.8 1. ]\n",
            " [7.7 3.8 6.7 2.2 1. ]\n",
            " [6.9 3.2 5.7 2.3 1. ]] [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
            "LINEAR CLASSIFIER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #1 - SPRAWDZENIE MNOŻENIA MACIERZOWEGO "
      ],
      "metadata": {
        "id": "AN5FQeafkKxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST #1')\n",
        "print('BY DEFAULT WEIGHT MATRIX IS NUMPY ONES, SO EVERY CLASS SHOULD HAVE THE SAME SCORE')\n",
        "print('INSTANCE:', training_x[0])\n",
        "print('SCORE:', classifier.get_score(training_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTHViSxAkhGB",
        "outputId": "927e0ca8-9630-4bca-aac9-f1ad153e0008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #1\n",
            "BY DEFAULT WEIGHT MATRIX IS NUMPY ONES, SO EVERY CLASS SHOULD HAVE THE SAME SCORE\n",
            "INSTANCE: [5.1 3.8 1.9 0.4 1. ]\n",
            "SCORE: [12.2 12.2 12.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #2 - CROSS-ENTROPY LOSS FUNCTION"
      ],
      "metadata": {
        "id": "kXh056gFkozC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST #2')\n",
        "print('CHECK MATH OPERATIONS OF THE CROSS-ENTROPY LOSS FUNCTION')\n",
        "cross_entropy_loss_function(training_x[0], classifier.W, training_y[0], debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4C8VA3jkrlp",
        "outputId": "32457555-55ff-4b16-ec33-36a22ae73798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #2\n",
            "CHECK MATH OPERATIONS OF THE CROSS-ENTROPY LOSS FUNCTION\n",
            "SCORE: [12.2 12.2 12.2]\n",
            "e^s:  [198789.15114295 198789.15114295 198789.15114295]\n",
            "sum of the e^s: 596367.4534288628\n",
            "normalized probability: 0.3333333333333333\n",
            "loss: 1.0986122886681098\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0986122886681098"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #3 - LOSS FUNCTION DLA ZESTAWU DANYCH"
      ],
      "metadata": {
        "id": "uADnfZ3wk1p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST #3')\n",
        "print('CHECK LOSS FUNCTION FOR THE DATASET')\n",
        "print('LOSS:', loss_function_for_the_dataset(training_x, classifier.W, training_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x81YvG6kyeR",
        "outputId": "fd8ed319-a60a-4f0b-fd0e-bdc057ea688b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #3\n",
            "CHECK LOSS FUNCTION FOR THE DATASET\n",
            "LOSS: 1.098612288668108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #4 - REGULARYZACJA L2"
      ],
      "metadata": {
        "id": "Ggj1Fg0vk2pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST #4')\n",
        "print('REGULARIZATION L2')\n",
        "print('CHECK LOSS WILL BE SMALLER FOR MORE \\'SPREAD OUT\\' WEIGHT MATRIX')\n",
        "w1 = np.zeros(classifier.W.shape)\n",
        "w1[:, 0] = 1.0\n",
        "print('W:', w1)\n",
        "print('LOSS:', loss_function_for_the_dataset(training_x, w1, training_y))\n",
        "print('LOSS L2:', loss_function_for_the_dataset_with_L2(training_x, w1, training_y))\n",
        "w2 = np.full(classifier.W.shape, 0.25)\n",
        "print('W:', w2)\n",
        "print('LOSS:', loss_function_for_the_dataset(training_x, w2, training_y))\n",
        "print('LOSS L2:', loss_function_for_the_dataset_with_L2(training_x, w2, training_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g9mboDLk5yH",
        "outputId": "abdf33da-3426-43db-856d-cdfdb78f5e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #4\n",
            "REGULARIZATION L2\n",
            "CHECK LOSS WILL BE SMALLER FOR MORE 'SPREAD OUT' WEIGHT MATRIX\n",
            "W: [[1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]]\n",
            "LOSS: 1.098612288668108\n",
            "LOSS L2: 1.128612288668108\n",
            "W: [[0.25 0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25 0.25]]\n",
            "LOSS: 1.098612288668108\n",
            "LOSS L2: 1.107987288668108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #5 - LOSOWE SZUKANIE MACIERZY WAG"
      ],
      "metadata": {
        "id": "wx4j2Wxqk78_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TEST #5\")\n",
        "print('NAIVE RANDOM WEIGHT MATRIX SEARCH')\n",
        "print('ALWAYS STARTS WITH NUMPY ONES WEIGHT MATRIX')\n",
        "for i in range(200, 1200, 200):\n",
        "    print(i, 'ITERATIONS')\n",
        "    w, loss = classifier.random_search(iterations=i)\n",
        "    accuracy, options = classifier.check_accuracy()\n",
        "    print('loss:', loss, 'accuracy:', accuracy)\n",
        "    print('chosen classes:', options)\n",
        "log_c = np.log(manager.get_classes_length())\n",
        "print('LOG(C):', log_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIH4pTNxk_l_",
        "outputId": "1b285e78-98c3-4030-cb2f-7e42adccee29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #5\n",
            "NAIVE RANDOM WEIGHT MATRIX SEARCH\n",
            "ALWAYS STARTS WITH NUMPY ONES WEIGHT MATRIX\n",
            "200 ITERATIONS\n",
            "loss: 1.0949814828881896 accuracy: 0.6333333333333333\n",
            "chosen classes: [9, 0, 21]\n",
            "400 ITERATIONS\n",
            "loss: 1.0954383956715634 accuracy: 0.3333333333333333\n",
            "chosen classes: [0, 0, 30]\n",
            "600 ITERATIONS\n",
            "loss: 1.0949100021082028 accuracy: 0.3333333333333333\n",
            "chosen classes: [0, 0, 30]\n",
            "800 ITERATIONS\n",
            "loss: 1.095404272513766 accuracy: 0.3333333333333333\n",
            "chosen classes: [0, 0, 30]\n",
            "1000 ITERATIONS\n",
            "loss: 1.0953411168358358 accuracy: 0.3333333333333333\n",
            "chosen classes: [0, 0, 30]\n",
            "LOG(C): 1.0986122886681098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #6 - GRADIENT DESCENT DLA 1000 ITERACJI (GRADIENT LICZONY NUMERYCZNIE)"
      ],
      "metadata": {
        "id": "6qohtAhtlBEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST #6')\n",
        "print('GRADIENT DESCENT WITH NUMERIC GRADIENT')\n",
        "print('1000 ITERATIONS')\n",
        "w, loss = classifier.gradient_descent()\n",
        "accuracy, options = classifier.check_accuracy()\n",
        "print('loss:', loss, 'accuracy:', accuracy)\n",
        "print('chosen classes:', options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2wuzRXElFp3",
        "outputId": "59b986c6-c682-4d41-96a6-a60f56131c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #6\n",
            "GRADIENT DESCENT WITH NUMERIC GRADIENT\n",
            "1000 ITERATIONS\n",
            "loss: 0.5248221911393842 accuracy: 1.0\n",
            "chosen classes: [10, 10, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEST #7 - ADAM DLA 1000 ITERACJI (GRADIENT LICZONY NUMERYCZNIE)"
      ],
      "metadata": {
        "id": "Wi7mrGt0lHNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('TEST #7')\n",
        "print('ADAM WITH NUMERIC GRADIENT')\n",
        "print('1000 ITERATIONS')\n",
        "w, loss = classifier.adam()\n",
        "accuracy, options = classifier.check_accuracy()\n",
        "print('loss:', loss, 'accuracy:', accuracy)\n",
        "print('chosen classes:', options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCL2TUrclG0P",
        "outputId": "1cc48634-f802-4698-97a8-5c695afe7cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST #7\n",
            "ADAM WITH NUMERIC GRADIENT\n",
            "1000 ITERATIONS\n",
            "loss: 0.5500518833670252 accuracy: 1.0\n",
            "chosen classes: [10, 10, 10]\n"
          ]
        }
      ]
    }
  ]
}